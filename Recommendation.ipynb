{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.functions import when,col,sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or get your Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Recommendation\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.cores\", \"8\") \\\n",
    "    .config(\"spark.default.parallelism\", \"8\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.3\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1024m\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"new_cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Customer_ID: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Income: string (nullable = true)\n",
      " |-- Customer_Segment: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- Total_Purchases: string (nullable = true)\n",
      " |-- Product_Category: string (nullable = true)\n",
      " |-- Product_Brand: string (nullable = true)\n",
      " |-- Product_Type: string (nullable = true)\n",
      " |-- Feedback: string (nullable = true)\n",
      " |-- Shipping_Method: string (nullable = true)\n",
      " |-- Payment_Method: string (nullable = true)\n",
      " |-- Ratings: string (nullable = true)\n",
      " |-- products: string (nullable = true)\n",
      " |-- Transaction_ID: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-------+---------------+---------+-------+-----------+-------+------+----------------+----------------+-------------+------------+---------------+--------------+--------+--------------+\n",
      "| Age|Income|  Year|  Month|Total_Purchases| Feedback|Ratings|Customer_ID|Country|Gender|Customer_Segment|Product_Category|Product_Brand|Product_Type|Shipping_Method|Payment_Method|products|Transaction_ID|\n",
      "+----+------+------+-------+---------------+---------+-------+-----------+-------+------+----------------+----------------+-------------+------------+---------------+--------------+--------+--------------+\n",
      "|26.0|   1.8|2023.0|   July|            3.0|     Good|    4.0|    22362.0|    0.0|   0.0|             0.0|             4.0|          7.0|        27.0|            1.0|           0.0|   245.0|      218625.0|\n",
      "|26.0|   1.8|2023.0|    May|            7.0|Excellent|    5.0|    22362.0|    0.0|   0.0|             0.0|             1.0|          1.0|         0.0|            0.0|           3.0|    10.0|      145963.0|\n",
      "|26.0|   1.8|2023.0|January|            5.0|     Good|    3.0|    22362.0|    0.0|   0.0|             0.0|             2.0|         11.0|        22.0|            0.0|           2.0|   279.0|       49995.0|\n",
      "|26.0|   1.8|2023.0|   July|            6.0|  Average|    2.0|    22362.0|    0.0|   0.0|             0.0|             0.0|          2.0|         9.0|            1.0|           1.0|   111.0|      210534.0|\n",
      "|46.0|   1.8|2023.0|October|            8.0|Excellent|    5.0|    11310.0|    1.0|   0.0|             0.0|             2.0|          3.0|        18.0|            0.0|           2.0|   182.0|       13653.0|\n",
      "+----+------+------+-------+---------------+---------+-------+-----------+-------+------+----------------+----------------+-------------+------------+---------------+--------------+--------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 19:52:51 WARN DAGScheduler: Broadcasting large task binary with size 10.6 MiB\n"
     ]
    }
   ],
   "source": [
    "# List of string columns to index (modify based on your actual use case)\n",
    "string_columns = [\n",
    "    \"Customer_ID\", \n",
    "    \"Country\", \n",
    "    \"Gender\", \n",
    "    \"Customer_Segment\",\n",
    "    \"Product_Category\", \n",
    "    \"Product_Brand\", \n",
    "    \"Product_Type\",\n",
    "    \"Shipping_Method\", \n",
    "    \"Payment_Method\",\n",
    "    \"products\", \n",
    "    \"Transaction_ID\"\n",
    "]\n",
    "\n",
    "# Create StringIndexer stages for each column\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=col + \"_index\", handleInvalid=\"keep\")\n",
    "    for col in string_columns\n",
    "]\n",
    "\n",
    "\n",
    "# Create and fit the Pipeline\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "indexed_df = pipeline.fit(df).transform(df)\n",
    "\n",
    "\n",
    "for col in string_columns:\n",
    "    indexed_df = indexed_df.drop(col).withColumnRenamed(f\"{col}_index\", col)\n",
    "\n",
    "indexed_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 19:53:35 WARN DAGScheduler: Broadcasting large task binary with size 10.6 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-------+---------------+-------+-----------+-------+------+----------------+----------------+-------------+------------+---------------+--------------+--------+--------------+---------------+\n",
      "| Age|Income|  Year|  Month|Total_Purchases|Ratings|Customer_ID|Country|Gender|Customer_Segment|Product_Category|Product_Brand|Product_Type|Shipping_Method|Payment_Method|products|Transaction_ID|Feedback_Rating|\n",
      "+----+------+------+-------+---------------+-------+-----------+-------+------+----------------+----------------+-------------+------------+---------------+--------------+--------+--------------+---------------+\n",
      "|26.0|   1.8|2023.0|   July|            3.0|    4.0|    22362.0|    0.0|   0.0|             0.0|             4.0|          7.0|        27.0|            1.0|           0.0|   245.0|      218625.0|            3.0|\n",
      "|26.0|   1.8|2023.0|    May|            7.0|    5.0|    22362.0|    0.0|   0.0|             0.0|             1.0|          1.0|         0.0|            0.0|           3.0|    10.0|      145963.0|            4.0|\n",
      "|26.0|   1.8|2023.0|January|            5.0|    3.0|    22362.0|    0.0|   0.0|             0.0|             2.0|         11.0|        22.0|            0.0|           2.0|   279.0|       49995.0|            3.0|\n",
      "|26.0|   1.8|2023.0|   July|            6.0|    2.0|    22362.0|    0.0|   0.0|             0.0|             0.0|          2.0|         9.0|            1.0|           1.0|   111.0|      210534.0|            2.0|\n",
      "|46.0|   1.8|2023.0|October|            8.0|    5.0|    11310.0|    1.0|   0.0|             0.0|             2.0|          3.0|        18.0|            0.0|           2.0|   182.0|       13653.0|            4.0|\n",
      "+----+------+------+-------+---------------+-------+-----------+-------+------+----------------+----------------+-------------+------------+---------------+--------------+--------+--------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexed_df = indexed_df.withColumn(\n",
    "    \"Feedback_Rating\",\n",
    "    when(col(\"Feedback\") == \"Bad\", 1.0)\n",
    "    .when(col(\"Feedback\") == \"Average\", 2.0)\n",
    "    .when(col(\"Feedback\") == \"Good\", 3.0)\n",
    "    .when(col(\"Feedback\") == \"Excellent\", 4.0)\n",
    "    .otherwise(None)\n",
    ")\n",
    "\n",
    "indexed_df = indexed_df.drop(\"Feedback\")\n",
    "indexed_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+-----+---------------+-------+-----------+-------+------+----------------+----------------+-------------+------------+---------------+--------------+--------+--------------+---------------+\n",
      "|Age|Income|Year|Month|Total_Purchases|Ratings|Customer_ID|Country|Gender|Customer_Segment|Product_Category|Product_Brand|Product_Type|Shipping_Method|Payment_Method|products|Transaction_ID|Feedback_Rating|\n",
      "+---+------+----+-----+---------------+-------+-----------+-------+------+----------------+----------------+-------------+------------+---------------+--------------+--------+--------------+---------------+\n",
      "|  0|     0|   0|    0|              0|      0|          0|      0|     0|               0|               0|            0|           0|              0|             0|       0|             0|              0|\n",
      "+---+------+----+-----+---------------+-------+-----------+-------+------+----------------+----------------+-------------+------------+---------------+--------------+--------+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_counts = indexed_df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in indexed_df.columns])\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_df = indexed_df.withColumn(\n",
    "    \"Month_Index\",\n",
    "    when((col(\"Month\") == \"January\") | (col(\"Month\") == \"1.0\"), 1)\n",
    "    .when((col(\"Month\") == \"February\") | (col(\"Month\") == \"2.0\"), 2)\n",
    "    .when((col(\"Month\") == \"March\") | (col(\"Month\") == \"3.0\"), 3)\n",
    "    .when((col(\"Month\") == \"April\") | (col(\"Month\") == \"4.0\"), 4)\n",
    "    .when((col(\"Month\") == \"May\") | (col(\"Month\") == \"5.0\"), 5)\n",
    "    .when((col(\"Month\") == \"June\") | (col(\"Month\") == \"6.0\"), 6)\n",
    "    .when((col(\"Month\") == \"July\") | (col(\"Month\") == \"7.0\"), 7)\n",
    "    .when((col(\"Month\") == \"August\") | (col(\"Month\") == \"8.0\"), 8)\n",
    "    .when((col(\"Month\") == \"September\") | (col(\"Month\") == \"9.0\"), 9)\n",
    "    .when((col(\"Month\") == \"October\") | (col(\"Month\") == \"10.0\"), 10)\n",
    "    .when((col(\"Month\") == \"November\") | (col(\"Month\") == \"11.0\"), 11)\n",
    "    .when((col(\"Month\") == \"December\") | (col(\"Month\") == \"12.0\"), 12)\n",
    "    .otherwise(0.0)\n",
    ")\n",
    "indexed_df = indexed_df.drop(\"Month\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+---------------+-------+-----------+-------+------+----------------+----------------+-------------+------------+---------------+--------------+--------+--------------+---------------+-----------+\n",
      "|Age|Income|Year|Total_Purchases|Ratings|Customer_ID|Country|Gender|Customer_Segment|Product_Category|Product_Brand|Product_Type|Shipping_Method|Payment_Method|products|Transaction_ID|Feedback_Rating|Month_Index|\n",
      "+---+------+----+---------------+-------+-----------+-------+------+----------------+----------------+-------------+------------+---------------+--------------+--------+--------------+---------------+-----------+\n",
      "|  0|     0|   0|              0|      0|          0|      0|     0|               0|               0|            0|           0|              0|             0|       0|             0|              0|          0|\n",
      "+---+------+----+---------------+-------+-----------+-------+------+----------------+----------------+-------------+------------+---------------+--------------+--------+--------------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_counts = indexed_df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in indexed_df.columns])\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+---------------+\n",
      "|Ratings|Feedback_Rating|Combined_Rating|\n",
      "+-------+---------------+---------------+\n",
      "|    4.0|            3.0|            3.5|\n",
      "|    5.0|            4.0|            4.5|\n",
      "|    3.0|            3.0|            3.0|\n",
      "|    2.0|            2.0|            2.0|\n",
      "|    5.0|            4.0|            4.5|\n",
      "+-------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cast numerical columns\n",
    "indexed_df = indexed_df.withColumn(\"Ratings\", col(\"Ratings\").cast(\"Double\"))\n",
    "indexed_df = indexed_df.withColumn(\"Age\", col(\"Age\").cast(\"Int\"))\n",
    "indexed_df = indexed_df.withColumn(\"Year\", col(\"Year\").cast(\"Int\"))\n",
    "indexed_df = indexed_df.withColumn(\"Income\", col(\"Income\").cast(\"Double\"))\n",
    "indexed_df = indexed_df.withColumn(\"Customer_ID\", col(\"Customer_ID\").cast(\"int\"))\n",
    "indexed_df = indexed_df.withColumn(\"Products\", col(\"products\").cast(\"int\"))\n",
    "\n",
    "# Weight parameter\n",
    "alpha = 0.5\n",
    "\n",
    "# Create a new column \"Combined_Rating\" as a weighted average of Ratings and Feedback_rating.\n",
    "indexed_df = indexed_df.withColumn(\n",
    "    \"Combined_Rating\",\n",
    "    alpha * col(\"Ratings\") + (1 - alpha) * col(\"Feedback_Rating\")\n",
    ")\n",
    "\n",
    "indexed_df.select(\"Ratings\", \"Feedback_Rating\", \"Combined_Rating\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+------+-------+------+----------------+--------------+----+-----------+--------------+---------------+---------------+--------+----------------+-------------+------------+-------+---------------+---------------+\n",
      "|Customer_ID|Age|Gender|Country|Income|Customer_Segment|Transaction_ID|Year|Month_Index|Payment_Method|Shipping_Method|Total_Purchases|Products|Product_Category|Product_Brand|Product_Type|Ratings|Feedback_Rating|Combined_Rating|\n",
      "+-----------+---+------+-------+------+----------------+--------------+----+-----------+--------------+---------------+---------------+--------+----------------+-------------+------------+-------+---------------+---------------+\n",
      "|      22362| 26|   0.0|    0.0|   1.8|             0.0|      218625.0|2023|        7.0|           0.0|            1.0|            3.0|     245|             4.0|          7.0|        27.0|    4.0|            3.0|            3.5|\n",
      "|      22362| 26|   0.0|    0.0|   1.8|             0.0|      145963.0|2023|        5.0|           3.0|            0.0|            7.0|      10|             1.0|          1.0|         0.0|    5.0|            4.0|            4.5|\n",
      "|      22362| 26|   0.0|    0.0|   1.8|             0.0|       49995.0|2023|        1.0|           2.0|            0.0|            5.0|     279|             2.0|         11.0|        22.0|    3.0|            3.0|            3.0|\n",
      "|      22362| 26|   0.0|    0.0|   1.8|             0.0|      210534.0|2023|        7.0|           1.0|            1.0|            6.0|     111|             0.0|          2.0|         9.0|    2.0|            2.0|            2.0|\n",
      "|      11310| 46|   0.0|    1.0|   1.8|             0.0|       13653.0|2023|       10.0|           2.0|            0.0|            8.0|     182|             2.0|          3.0|        18.0|    5.0|            4.0|            4.5|\n",
      "+-----------+---+------+-------+------+----------------+--------------+----+-----------+--------------+---------------+---------------+--------+----------------+-------------+------------+-------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 19:53:51 WARN DAGScheduler: Broadcasting large task binary with size 10.6 MiB\n"
     ]
    }
   ],
   "source": [
    "reordered_cols = [\n",
    "    \"Customer_ID\", \n",
    "    \"Age\", \n",
    "    \"Gender\", \n",
    "    \"Country\", \n",
    "    \"Income\", \n",
    "    \"Customer_Segment\",\n",
    "    \"Transaction_ID\", \n",
    "    \"Year\", \n",
    "    \"Month_Index\", \n",
    "    \"Payment_Method\", \n",
    "    \"Shipping_Method\", \n",
    "    \"Total_Purchases\", \n",
    "    \"Products\",\n",
    "    \"Product_Category\", \n",
    "    \"Product_Brand\", \n",
    "    \"Product_Type\",\n",
    "    \"Ratings\", \n",
    "    \"Feedback_Rating\",\n",
    "    \"Combined_Rating\"\n",
    "]\n",
    "\n",
    "# Reorder the columns in the DataFrame\n",
    "indexed_df = indexed_df.select(reordered_cols)\n",
    "\n",
    "# Show the result\n",
    "indexed_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+---+------+------+----------------+----+-----+---------------+----------------+-------------+------------+--------+---------------+--------------+-------+--------+--------------+\n",
      "|Customer_ID|Country|Age|Gender|Income|Customer_Segment|Year|Month|Total_Purchases|Product_Category|Product_Brand|Product_Type|Feedback|Shipping_Method|Payment_Method|Ratings|products|Transaction_ID|\n",
      "+-----------+-------+---+------+------+----------------+----+-----+---------------+----------------+-------------+------------+--------+---------------+--------------+-------+--------+--------------+\n",
      "|          0|      0|  0|     0|     0|               0|   0|    0|              0|               0|            0|           0|       0|              0|             0|      0|       0|             0|\n",
      "+-----------+-------+---+------+------+----------------+----+-----+---------------+----------------+-------------+------------+--------+---------------+--------------+-------+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_counts = df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation Phase 1\n",
    "\n",
    "1. Generate Recommendations with Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = indexed_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Initialize the ALS (Alternating Least Squares) model.\n",
    "als = ALS(\n",
    "    userCol=\"Customer_ID\",  \n",
    "    itemCol=\"Products\",      \n",
    "    ratingCol=\"Ratings\",     \n",
    "    rank=10,                  \n",
    "    maxIter=10,               \n",
    "    regParam=0.1,             \n",
    "    coldStartStrategy=\"drop\"  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 19:53:52 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:53:53 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:53:54 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:53:55 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:53:55 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:53:56 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:53:57 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:53:58 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:53:58 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:53:59 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:53:59 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:00 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:01 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:01 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:02 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:03 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:03 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:04 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:05 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:05 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:06 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:06 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:07 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:07 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:08 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:08 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:09 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:09 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:10 WARN DAGScheduler: Broadcasting large task binary with size 12.6 MiB\n",
      "[Stage 642:===================================================>  (95 + 5) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------------------------------------------------------------------------------+\n",
      "|Customer_ID|recommendations                                                                         |\n",
      "+-----------+----------------------------------------------------------------------------------------+\n",
      "|12         |[{124, 4.6717706}, {40, 4.399662}, {175, 4.3472805}, {267, 4.3231187}, {69, 4.208994}]  |\n",
      "|13         |[{174, 5.0660167}, {248, 4.833373}, {302, 4.71884}, {36, 4.712051}, {159, 4.6419716}]   |\n",
      "|14         |[{124, 4.5577626}, {310, 4.3268714}, {72, 4.218303}, {122, 4.086638}, {77, 4.0722814}]  |\n",
      "|18         |[{14, 3.7602866}, {231, 3.7069128}, {155, 3.5947292}, {200, 3.4845002}, {22, 3.4713295}]|\n",
      "|38         |[{171, 4.3633294}, {288, 4.1702466}, {227, 4.082798}, {299, 4.066179}, {88, 4.0311594}] |\n",
      "+-----------+----------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 19:54:15 WARN DAGScheduler: Broadcasting large task binary with size 12.6 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = als.fit(train)\n",
    "predictions = model.transform(test)\n",
    "userRecs = model.recommendForAllUsers(5)\n",
    "userRecs.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 19:54:16 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:16 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:16 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:17 WARN DAGScheduler: Broadcasting large task binary with size 12.6 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error of 1 (RMSE): 2.075946774027161\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",            \n",
    "    labelCol=\"Ratings\",     \n",
    "    predictionCol=\"prediction\"    \n",
    ")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error of 1 (RMSE):\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Generate Recommendation with Feedbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 19:54:18 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:19 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:20 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:20 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:21 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:21 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:22 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:22 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:23 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:23 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:23 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:24 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:24 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:25 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:25 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:25 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:26 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:26 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:27 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:27 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:28 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:28 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:29 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:29 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:30 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:30 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:31 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:31 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:32 WARN DAGScheduler: Broadcasting large task binary with size 12.6 MiB\n",
      "25/04/16 19:54:37 WARN DAGScheduler: Broadcasting large task binary with size 12.6 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------------------------------------------------------------------------+\n",
      "|Customer_ID|recommendations                                                                           |\n",
      "+-----------+------------------------------------------------------------------------------------------+\n",
      "|12         |[{124, 3.8721766}, {122, 3.867613}, {128, 3.8306646}, {49, 3.7383}, {120, 3.6640534}]     |\n",
      "|13         |[{36, 4.2894664}, {127, 4.2145014}, {159, 4.189936}, {195, 4.1831694}, {292, 4.1579185}]  |\n",
      "|14         |[{124, 3.8338265}, {122, 3.6338186}, {69, 3.443578}, {166, 3.3902285}, {77, 3.366441}]    |\n",
      "|18         |[{155, 3.3747208}, {231, 3.1471965}, {200, 3.1333094}, {273, 3.1143727}, {122, 3.1120393}]|\n",
      "|38         |[{171, 3.5413203}, {88, 3.4155073}, {288, 3.4029078}, {299, 3.36233}, {226, 3.345981}]    |\n",
      "+-----------+------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.stop()\n",
    "\n",
    "als_fb = ALS(\n",
    "    userCol=\"Customer_ID\",   \n",
    "    itemCol=\"Products\",      \n",
    "    ratingCol=\"Feedback_Rating\",   \n",
    "    rank=10,                       \n",
    "    maxIter=10,                    \n",
    "    regParam=0.1,                  \n",
    "    coldStartStrategy=\"drop\"       \n",
    ")\n",
    "\n",
    "als_model_fb = als_fb.fit(train)\n",
    "userRecs = als_model_fb.recommendForAllUsers(5)\n",
    "userRecs.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 19:54:38 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:38 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:38 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error of 1 (RMSE): 1.6128865285338405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 19:54:39 WARN DAGScheduler: Broadcasting large task binary with size 12.6 MiB\n"
     ]
    }
   ],
   "source": [
    "predictions = als_model_fb.transform(test)\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",            \n",
    "    labelCol=\"Feedback_Rating\",     \n",
    "    predictionCol=\"prediction\"    \n",
    ")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error of 1 (RMSE):\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Generate Recommendations with Combined Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 19:54:40 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:41 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:42 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:42 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:43 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:43 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:44 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:44 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:45 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:46 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:46 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:47 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:48 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:48 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:49 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:49 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:50 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:50 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:51 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:51 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:52 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:52 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:53 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:53 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:54 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:54 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:55 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:55 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:54:56 WARN DAGScheduler: Broadcasting large task binary with size 12.6 MiB\n",
      "25/04/16 19:55:01 WARN DAGScheduler: Broadcasting large task binary with size 12.6 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------------------------------------------------------------------------+\n",
      "|Customer_ID|recommendations                                                                          |\n",
      "+-----------+-----------------------------------------------------------------------------------------+\n",
      "|12         |[{124, 4.2506056}, {122, 3.9849882}, {40, 3.969384}, {128, 3.9380352}, {267, 3.8983276}] |\n",
      "|13         |[{174, 4.621201}, {36, 4.4970493}, {159, 4.4098015}, {127, 4.380688}, {248, 4.3614435}]  |\n",
      "|14         |[{124, 4.1920805}, {122, 3.85035}, {77, 3.7608159}, {310, 3.6747696}, {72, 3.645151}]    |\n",
      "|18         |[{155, 3.4979124}, {231, 3.4545639}, {14, 3.348528}, {200, 3.3011057}, {273, 3.29994}]   |\n",
      "|38         |[{171, 3.9489934}, {288, 3.7822936}, {88, 3.7263923}, {299, 3.7139812}, {226, 3.6598186}]|\n",
      "+-----------+-----------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 19:55:01 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:55:02 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 19:55:02 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error of 1 (RMSE): 1.8215680342473874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 19:55:03 WARN DAGScheduler: Broadcasting large task binary with size 12.6 MiB\n"
     ]
    }
   ],
   "source": [
    "als_combined = ALS(\n",
    "    userCol=\"Customer_ID\",   \n",
    "    itemCol=\"Products\",      \n",
    "    ratingCol=\"Combined_Rating\",   \n",
    "    rank=10,                       \n",
    "    maxIter=10,                    \n",
    "    regParam=0.1,                  \n",
    "    coldStartStrategy=\"drop\"       \n",
    ")\n",
    "\n",
    "als_model_combined = als_combined.fit(train)\n",
    "userRecs = als_model_combined.recommendForAllUsers(5)\n",
    "userRecs.show(5, truncate=False)\n",
    "\n",
    "predictions = als_model_combined.transform(test)\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",            \n",
    "    labelCol=\"Combined_Rating\",     \n",
    "    predictionCol=\"prediction\"    \n",
    ")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error of 1 (RMSE):\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation Phase 2\n",
    "\n",
    "1. User Profile Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 19:55:03 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------------+\n",
      "|Customer_ID|user_profile_vector           |\n",
      "+-----------+------------------------------+\n",
      "|38664      |[38664.0,26.0,1.0,2.0,2.0,0.0]|\n",
      "|58182      |[58182.0,38.0,0.0,0.0,1.0,0.0]|\n",
      "|22370      |[22370.0,31.0,1.0,4.0,1.5,0.0]|\n",
      "|4925       |[4925.0,20.0,0.0,3.0,2.2,1.0] |\n",
      "|58188      |[58188.0,30.0,0.0,0.0,2.0,0.0]|\n",
      "+-----------+------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 19:55:04 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "distinct_users = indexed_df.select(\"Customer_ID\", \"Age\", \"Gender\", \"Country\", \"Income\", \"Customer_Segment\").distinct()\n",
    "\n",
    "user_profile_vector = VectorAssembler(\n",
    "    inputCols=[\"Customer_ID\",\"Age\", \"Gender\", \"Country\", \"Income\", \"Customer_Segment\"],\n",
    "    outputCol=\"user_profile_vector\"\n",
    ").transform(distinct_users)\n",
    "\n",
    "\n",
    "user_profile_vector.select(\"Customer_ID\", \"user_profile_vector\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Product Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------------+\n",
      "|Products|product_vector               |\n",
      "+--------+-----------------------------+\n",
      "|245     |[218625.0,4.0,7.0,27.0,245.0]|\n",
      "|10      |[145963.0,1.0,1.0,0.0,10.0]  |\n",
      "|279     |[49995.0,2.0,11.0,22.0,279.0]|\n",
      "|111     |[210534.0,0.0,2.0,9.0,111.0] |\n",
      "|182     |[13653.0,2.0,3.0,18.0,182.0] |\n",
      "+--------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 19:55:04 WARN DAGScheduler: Broadcasting large task binary with size 8.3 MiB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "product_vector = VectorAssembler(\n",
    "    inputCols=[\"Transaction_ID\",\"Product_Category\", \"Product_Brand\", \"Product_Type\",\"Products\"],\n",
    "    outputCol=\"product_vector\"\n",
    ").transform(indexed_df)\n",
    "\n",
    "\n",
    "product_vector.select(\"Products\", \"product_vector\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Transaction Vector  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------------------------+\n",
      "|Transaction_ID|transaction_vector               |\n",
      "+--------------+---------------------------------+\n",
      "|218625.0      |[218625.0,22362.0,7.0,2023.0,3.5]|\n",
      "|145963.0      |[145963.0,22362.0,5.0,2023.0,4.5]|\n",
      "|49995.0       |[49995.0,22362.0,1.0,2023.0,3.0] |\n",
      "|210534.0      |[210534.0,22362.0,7.0,2023.0,2.0]|\n",
      "|13653.0       |[13653.0,11310.0,10.0,2023.0,4.5]|\n",
      "+--------------+---------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 19:55:05 WARN DAGScheduler: Broadcasting large task binary with size 10.6 MiB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transaction_vector = VectorAssembler(\n",
    "    inputCols=[\"Transaction_ID\",\"Customer_ID\",\"Month_Index\",\"Year\",\"Combined_Rating\"],\n",
    "    outputCol=\"transaction_vector\"\n",
    ").transform(indexed_df)\n",
    "\n",
    "\n",
    "transaction_vector.select(\"Transaction_ID\", \"transaction_vector\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 19:55:05 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/04/16 19:55:05 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/04/16 19:55:06 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/04/16 19:55:06 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------------------------------------------------------------+\n",
      "|Customer_ID|user_profile_scaled                                                       |\n",
      "+-----------+--------------------------------------------------------------------------+\n",
      "|38664      |[0.4457253527621507,0.15384615384615385,1.0,0.5,0.5,0.0]                  |\n",
      "|58182      |(6,[0,1],[0.6707322696670663,0.38461538461538464])                        |\n",
      "|22370      |[0.2578852716037997,0.25,1.0,1.0,0.25,0.0]                                |\n",
      "|4925       |[0.05677626118232961,0.038461538461538464,0.0,0.75,0.6000000000000001,0.5]|\n",
      "|58188      |[0.6708014387162224,0.23076923076923078,0.0,0.0,0.5,0.0]                  |\n",
      "+-----------+--------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 19:55:06 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "scaler_user = MinMaxScaler(inputCol=\"user_profile_vector\", outputCol=\"user_profile_scaled\")\n",
    "scaler_model_user = scaler_user.fit(user_profile_vector)\n",
    "user_profile_scaled = scaler_model_user.transform(user_profile_vector)\n",
    "\n",
    "user_profile_scaled.select(\"Customer_ID\", \"user_profile_scaled\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 19:55:06 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "25/04/16 19:55:07 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------------------------------------------------------+\n",
      "|Products|product_vector_scaled                                                  |\n",
      "+--------+-----------------------------------------------------------------------+\n",
      "|245     |[0.7259189162267159,0.8,0.38888888888888884,0.84375,0.7728706624605678]|\n",
      "|10      |[0.48465318590829104,0.2,0.05555555555555555,0.0,0.031545741324921134] |\n",
      "|279     |[0.16600258989939237,0.4,0.611111111111111,0.6875,0.8801261829652997]  |\n",
      "|111     |[0.6990536906066341,0.0,0.1111111111111111,0.28125,0.3501577287066246] |\n",
      "|182     |[0.04533320051797988,0.4,0.16666666666666666,0.5625,0.5741324921135647]|\n",
      "+--------+-----------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 19:55:08 WARN DAGScheduler: Broadcasting large task binary with size 8.3 MiB\n"
     ]
    }
   ],
   "source": [
    "# Then apply scaling\n",
    "scaler_product = MinMaxScaler(inputCol=\"product_vector\", outputCol=\"product_vector_scaled\")\n",
    "scaler_model_product = scaler_product.fit(product_vector)\n",
    "product_scaled = scaler_model_product.transform(product_vector)\n",
    "\n",
    "product_scaled.select(\"Products\", \"product_vector_scaled\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 19:55:26 WARN DAGScheduler: Broadcasting large task binary with size 12.4 MiB\n",
      "25/04/16 19:55:27 WARN DAGScheduler: Broadcasting large task binary with size 12.4 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------------------------------------------------------------------------+\n",
      "|Transaction_ID|transaction_vector_scaled                                                        |\n",
      "+--------------+---------------------------------------------------------------------------------+\n",
      "|218625.0      |[0.7259189162267159,0.2577930462049248,0.5454545454545454,0.0,0.7142857142857142]|\n",
      "|145963.0      |[0.48465318590829104,0.2577930462049248,0.36363636363636365,0.0,1.0]             |\n",
      "|49995.0       |[0.16600258989939237,0.2577930462049248,0.0,0.0,0.5714285714285714]              |\n",
      "|210534.0      |[0.6990536906066341,0.2577930462049248,0.5454545454545454,0.0,0.2857142857142857]|\n",
      "|13653.0       |[0.04533320051797988,0.13038365765931936,0.8181818181818182,0.0,1.0]             |\n",
      "+--------------+---------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 19:55:27 WARN DAGScheduler: Broadcasting large task binary with size 10.6 MiB\n"
     ]
    }
   ],
   "source": [
    "# Then apply scaling\n",
    "scaler_transaction = MinMaxScaler(inputCol=\"transaction_vector\", outputCol=\"transaction_vector_scaled\")\n",
    "scaler_model_transaction = scaler_transaction.fit(transaction_vector)\n",
    "transaction_vector_scaled = scaler_model_transaction.transform(transaction_vector)\n",
    "\n",
    "transaction_vector_scaled.select(\"Transaction_ID\", \"transaction_vector_scaled\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 20:44:58 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/04/16 20:44:58 WARN DAGScheduler: Broadcasting large task binary with size 8.3 MiB\n",
      "25/04/16 20:44:59 WARN DAGScheduler: Broadcasting large task binary with size 10.6 MiB\n",
      "25/04/16 20:45:00 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "|Customer_ID|Products|hybrid_vector                                                                                                                                                                                                                 |Combined_Rating|\n",
      "+-----------+--------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "|38664      |29      |[0.4457253527621507,0.15384615384615385,1.0,0.5,0.5,0.0,0.8020984825845868,0.4457253527621507,0.8181818181818182,0.0,1.0,0.8020984825845868,0.6000000000000001,0.5,0.0625,0.0914826498422713]                                 |4.5            |\n",
      "|38664      |36      |[0.4457253527621507,0.15384615384615385,1.0,0.5,0.5,0.0,0.26706511272703126,0.4457253527621507,0.5454545454545454,1.0,0.8571428571428571,0.26706511272703126,0.6000000000000001,0.5555555555555556,0.0625,0.11356466876971609]|4.0            |\n",
      "|38664      |73      |[0.4457253527621507,0.15384615384615385,1.0,0.5,0.5,0.0,0.5269615167513365,0.4457253527621507,0.7272727272727273,0.0,0.5714285714285714,0.5269615167513365,0.8,0.7777777777777777,0.21875,0.23028391167192427]                |3.0            |\n",
      "|58182      |74      |[0.6707322696670663,0.38461538461538464,0.0,0.0,0.0,0.0,0.33634824185675866,0.6707322696670663,0.5454545454545454,0.0,0.0,0.33634824185675866,0.2,0.05555555555555555,0.3125,0.2334384858044164]                              |1.0            |\n",
      "|58182      |12      |[0.6707322696670663,0.38461538461538464,0.0,0.0,0.0,0.0,0.7555732642693496,0.6707322696670663,0.8181818181818182,0.0,0.2857142857142857,0.7555732642693496,0.6000000000000001,0.5,0.09375,0.03785488958990536]                |2.0            |\n",
      "+-----------+--------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make sure all the necessary columns are selected\n",
    "user_profile_scaled = user_profile_scaled.select(\"Customer_ID\", \"user_profile_scaled\")\n",
    "product_scaled = product_scaled.select(\"Transaction_ID\", \"Products\", \"product_vector_scaled\")\n",
    "transaction_scaled = transaction_vector_scaled.select(\n",
    "    \"Transaction_ID\", \"Customer_ID\", \"transaction_vector_scaled\", \"Combined_Rating\"\n",
    ")# Join user and transaction data on Customer_ID\n",
    "user_transaction_joined = transaction_scaled.join(user_profile_scaled, on=\"Customer_ID\", how=\"inner\")\n",
    "\n",
    "# Now join with product data on Transaction_ID\n",
    "hybrid_joined = user_transaction_joined.join(product_scaled, on=\"Transaction_ID\", how=\"inner\")\n",
    "\n",
    "hybrid_assembler = VectorAssembler(\n",
    "    inputCols=[\"user_profile_scaled\", \"transaction_vector_scaled\", \"product_vector_scaled\"],\n",
    "    outputCol=\"hybrid_vector\"\n",
    ")\n",
    "\n",
    "hybrid_df = hybrid_assembler.transform(hybrid_joined)\n",
    "hybrid_df.select(\"Customer_ID\", \"Products\", \"hybrid_vector\", \"Combined_Rating\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 20:45:07 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/04/16 20:45:07 WARN DAGScheduler: Broadcasting large task binary with size 8.3 MiB\n",
      "25/04/16 20:45:08 WARN DAGScheduler: Broadcasting large task binary with size 10.6 MiB\n",
      "25/04/16 20:45:09 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:45:10 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "[Stage 1439:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Products|product_vector                                                                                                                                                                                                                                                                                                            |\n",
      "+--------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|12      |[0.35991731116730563,0.3066009656723346,0.43723129836629404,0.28579105760963025,0.4674548581255388,0.26268271711092006,0.48727876015554294,0.35991731116730563,0.46865473305713923,0.1706792777300086,0.5622159439872293,0.48727876015554294,0.6005159071367028,0.46402980796789145,0.2579535683576956,0.0378548895899074]|\n",
      "|13      |[0.35989469613428404,0.30723021865660044,0.439161554192229,0.2877044989775051,0.46533742331288314,0.27044989775051126,0.48855474555844497,0.35989469613428404,0.4638408626138669,0.17893660531697342,0.5652205667543119,0.48855474555844497,0.0,0.365882753919559,0.03125,0.041009463722398054]                           |\n",
      "|14      |[0.3625434240080224,0.31337228172671344,0.4177215189873418,0.2894251054852321,0.4671677215189874,0.27874472573839665,0.4834370865719341,0.3625434240080224,0.4660049865746048,0.15770042194092826,0.5520644966847527,0.4834370865719341,0.6002109704641403,0.4221167369901456,0.0625,0.0441640378548891]                  |\n",
      "|18      |[0.353926210944802,0.30986586072700323,0.4117330462863294,0.30072658772874056,0.46741119483315396,0.26883745963401506,0.49339004684743476,0.353926210944802,0.46648400039142646,0.1689989235737352,0.5764262648008646,0.49339004684743476,5.382131324004305E-4,0.3812343021169666,0.03125,0.05678233438485953]            |\n",
      "|38      |[0.3607871890757087,0.30865878838466104,0.4066247858366648,0.2972587093089663,0.4709308966304958,0.2781267846944603,0.49382457530422863,0.3607871890757087,0.46669435647162516,0.17532838378069673,0.5539691604797282,0.49382457530422863,0.6009137635636911,0.4324512976711646,0.0625,0.11987381703469568]               |\n",
      "|46      |[0.3664105574421029,0.3001313618867249,0.40984908657664815,0.3018268467037331,0.4749007148530577,0.25178713264495634,0.4902150793794598,0.3664105574421029,0.4530290995739759,0.1938046068308181,0.5668898218540783,0.4902150793794598,0.19999999999999543,0.026961433236254866,0.3125,0.14511041009463768]               |\n",
      "|67      |[0.3507884295276287,0.3101822851415696,0.4161237785016287,0.30191368078175895,0.4718648208469047,0.2780944625407166,0.4792648491729161,0.3507884295276287,0.48541604974829694,0.15390879478827363,0.5658445788738943,0.4792648491729161,0.0016286644951140066,0.4223669923995625,0.28125,0.21135646687696935]             |\n",
      "|70      |[0.355074220657582,0.3085277427490554,0.4081967213114754,0.2875,0.466188524590164,0.2725409836065574,0.4781094659788571,0.355074220657582,0.4742175856929953,0.16229508196721312,0.5599531615925055,0.4781094659788571,8.19672131147541E-4,0.19945355191256733,0.15625,0.2208201892744409]                                |\n",
      "|93      |[0.36716814388088354,0.3018184744126178,0.41422594142259417,0.27468619246861925,0.46167364016736356,0.2564853556485356,0.4913378715617181,0.36716814388088354,0.4747812856599456,0.1682008368200837,0.5564853556485359,0.4913378715617181,0.7999999999999832,0.6142259414226046,0.21875,0.2933753943217623]               |\n",
      "|107     |[0.34645515185417297,0.30734919286321305,0.4367034834324554,0.3005522514868309,0.46329651656754467,0.2574341546304163,0.4718540117783071,0.34645515185417297,0.4572487835019687,0.16737468139337297,0.5527369826435243,0.4718540117783071,0.1999999999999959,0.03006702539412858,0.3125,0.33753943217665455]              |\n",
      "|148     |[0.35510160402347885,0.3365830460111762,0.3956723338485317,0.2979134466769706,0.4555641421947449,0.31839258114374036,0.4877328701423541,0.35510160402347885,0.4775888717156106,0.1545595054095827,0.5553102230072849,0.4877328701423541,0.8003091190108265,0.3888888888888862,0.46875,0.46687697160882785]                |\n",
      "|157     |[0.3592248094969105,0.3116286057692309,0.471875,0.272265625,0.47265625000000017,0.25546875,0.4954444904455954,0.3592248094969105,0.4961647727272722,0.128125,0.562499999999998,0.4954444904455954,0.400000000000004,0.166666666666668,0.5625,0.4952681388012591]                                                          |\n",
      "|161     |[0.35811996497461757,0.3092923559956529,0.40345368916797486,0.2731554160125589,0.46405023547880647,0.2692307692307692,0.47563510680924126,0.35811996497461757,0.4558298844013123,0.1836734693877551,0.5503476115721001,0.47563510680924126,0.8000000000000079,0.7784754927612016,0.71875,0.5078864353312325]              |\n",
      "|171     |[0.377594517471288,0.319096245566834,0.465818759936407,0.26947535771065184,0.4786168521462639,0.2575516693163752,0.4664150037429542,0.377594517471288,0.48056077467842145,0.15262321144674085,0.5718828071769233,0.4664150037429542,0.8003179650238553,0.3888888888888862,0.8125,0.5394321766561511]                      |\n",
      "|186     |[0.35109456947351264,0.30082639492978797,0.4281098546042003,0.26978998384491115,0.4722132471728597,0.27221324717285944,0.47820949025778525,0.35109456947351264,0.48142164781906255,0.13731825525040386,0.571659358412184,0.47820949025778525,0.6000000000000043,0.5,0.53125,0.586750788643538]                            |\n",
      "|190     |[0.35710765803060507,0.30831365726357723,0.3974151857835218,0.30048465266558966,0.4581583198707592,0.27059773828756056,0.505297675092985,0.35710765803060507,0.46174181230723976,0.1583198707592892,0.5624278790676192,0.505297675092985,0.40000000000000396,0.16666666666666788,0.59375,0.5993690851735053]              |\n",
      "|198     |[0.3613780507436996,0.3246166313427258,0.3987034035656402,0.27957860615883307,0.4638573743922203,0.26661264181523503,0.469434876590697,0.3613780507436996,0.4840135553263589,0.15883306320907617,0.5654086594118996,0.469434876590697,0.6006482982171841,0.22222222222222526,0.4375,0.6246056782334365]                   |\n",
      "|202     |[0.38092522029837644,0.3004924572995889,0.42787682333873583,0.2779578606158833,0.47706645056726116,0.2625607779578606,0.48039443038707647,0.38092522029837644,0.4689848239280971,0.1507293354943274,0.5378559851817546,0.48039443038707647,0.6000000000000042,0.223482802088964,0.4375,0.6372239747634023]                |\n",
      "|203     |[0.35122326080012467,0.28219673357436714,0.40842787682333875,0.303484602917342,0.458752025931929,0.24878444084278767,0.47989820790220084,0.35122326080012467,0.4766465301311329,0.1912479740680713,0.5540634406112518,0.47989820790220084,0.8003241491085978,0.7777777777777724,0.71875,0.6403785488959058]               |\n",
      "|218     |[0.3457848646025313,0.31248423707440165,0.4245901639344262,0.30368852459016393,0.4659836065573767,0.2573770491803279,0.4655178574053003,0.3457848646025313,0.4524590163934423,0.17868852459016393,0.5859484777517552,0.4655178574053003,0.4000000000000039,0.16666666666666782,0.5625,0.6876971608832746]                 |\n",
      "+--------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "from pyspark.ml.linalg import DenseVector, VectorUDT\n",
    "import numpy as np\n",
    "\n",
    "user_vectors = hybrid_df.select(\"Customer_ID\", \"hybrid_vector\").distinct()\n",
    "\n",
    "# 1. Önce vektörleri aynı product altında toplayalım\n",
    "vector_list_df = hybrid_df.groupBy(\"Products\").agg(\n",
    "    F.first(\"Transaction_ID\").alias(\"Transaction_ID\"),\n",
    "    F.first(\"Customer_ID\").alias(\"Customer_ID\"),\n",
    "    F.collect_list(\"hybrid_vector\").alias(\"hybrid_vector_list\")\n",
    ")\n",
    "\n",
    "# 2. UDF: Vektörleri ortalamak için Python fonksiyonu\n",
    "def average_vectors(vectors):\n",
    "    if not vectors:\n",
    "        return None\n",
    "    arrays = [np.array(v.toArray()) for v in vectors]\n",
    "    avg_array = np.mean(arrays, axis=0)\n",
    "    return avg_array.tolist()\n",
    "\n",
    "average_vectors_udf = F.udf(average_vectors, ArrayType(DoubleType()))\n",
    "\n",
    "# 3. Ortalamasını al\n",
    "vector_list_df = vector_list_df.withColumn(\n",
    "    \"avg_vector_array\", average_vectors_udf(\"hybrid_vector_list\")\n",
    ")\n",
    "\n",
    "# 4. (Opsiyonel) Tekrar DenseVector'e çevir\n",
    "def array_to_vector(arr):\n",
    "    if arr is None:\n",
    "        return None\n",
    "    return DenseVector(arr)\n",
    "\n",
    "array_to_vector_udf = F.udf(array_to_vector, VectorUDT())\n",
    "\n",
    "# 5. Son hali\n",
    "product_vectors = vector_list_df.withColumn(\n",
    "    \"product_vector\", array_to_vector_udf(\"avg_vector_array\")\n",
    ").select(\"Products\", \"product_vector\")\n",
    "\n",
    "product_vectors.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    if vec1 is None or vec2 is None:\n",
    "        return None\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "    return float(np.dot(vec1, vec2) / (norm1 * norm2))\n",
    "\n",
    "cosine_sim_udf = udf(cosine_similarity, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross join: every user with every product\n",
    "user_product_scores = user_vectors.crossJoin(product_vectors)\n",
    "\n",
    "# Add cosine similarity\n",
    "user_product_scores = user_product_scores.withColumn(\n",
    "    \"similarity\", cosine_sim_udf(\"hybrid_vector\", \"product_vector\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 20:45:12 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/04/16 20:45:13 WARN DAGScheduler: Broadcasting large task binary with size 8.3 MiB\n",
      "25/04/16 20:45:13 WARN DAGScheduler: Broadcasting large task binary with size 8.3 MiB\n",
      "25/04/16 20:45:14 WARN DAGScheduler: Broadcasting large task binary with size 10.6 MiB\n",
      "25/04/16 20:45:15 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:45:15 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:45:17 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:50:40 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------------------+\n",
      "|Customer_ID|Products|similarity        |\n",
      "+-----------+--------+------------------+\n",
      "|12         |212     |0.8861749803080519|\n",
      "|12         |213     |0.8837212971417542|\n",
      "|12         |242     |0.8815541763986517|\n",
      "|12         |239     |0.8814559570708499|\n",
      "|12         |269     |0.881112519992375 |\n",
      "|13         |269     |0.9080098769164253|\n",
      "|13         |213     |0.9077187307490607|\n",
      "|13         |292     |0.9048125536952083|\n",
      "|13         |236     |0.9028009729000578|\n",
      "|13         |275     |0.9026139564873609|\n",
      "|14         |184     |0.8930218505842851|\n",
      "|14         |181     |0.8905888486812419|\n",
      "|14         |203     |0.8903263974207459|\n",
      "|14         |205     |0.8889383055353763|\n",
      "|14         |215     |0.8842558192230076|\n",
      "|18         |286     |0.8985492299457141|\n",
      "|18         |243     |0.8975734158204195|\n",
      "|18         |315     |0.8969658103699043|\n",
      "|18         |239     |0.896063901513035 |\n",
      "|18         |212     |0.8958413647800829|\n",
      "+-----------+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "windowSpec = Window.partitionBy(\"Customer_ID\").orderBy(F.desc(\"similarity\"))\n",
    "\n",
    "top_n_recommendations = user_product_scores.withColumn(\"rank\", row_number().over(windowSpec)) \\\n",
    "    .filter(\"rank <= 5\")\n",
    "\n",
    "top_n_recommendations.select(\"Customer_ID\", \"Products\", \"similarity\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 20:50:41 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/04/16 20:50:41 WARN DAGScheduler: Broadcasting large task binary with size 8.3 MiB\n",
      "25/04/16 20:50:42 WARN DAGScheduler: Broadcasting large task binary with size 10.6 MiB\n",
      "25/04/16 20:50:43 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:50:44 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/04/16 20:50:44 WARN DAGScheduler: Broadcasting large task binary with size 8.3 MiB\n",
      "25/04/16 20:50:45 WARN DAGScheduler: Broadcasting large task binary with size 10.6 MiB\n",
      "25/04/16 20:50:46 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:50:47 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:50:48 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:50:48 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/04/16 20:50:49 WARN DAGScheduler: Broadcasting large task binary with size 8.3 MiB\n",
      "25/04/16 20:50:50 WARN DAGScheduler: Broadcasting large task binary with size 10.6 MiB\n",
      "25/04/16 20:50:50 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:50:52 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:50:53 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:50:53 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:50:54 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:50:55 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:50:55 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:50:56 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:50:56 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:50:57 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:50:57 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:50:57 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:50:58 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:50:59 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:50:59 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:51:00 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:51:00 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:51:00 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:51:01 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:51:01 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:51:02 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:51:02 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:51:03 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:51:03 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:51:04 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:51:04 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:51:04 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:51:05 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:51:06 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/04/16 20:51:06 WARN DAGScheduler: Broadcasting large task binary with size 8.3 MiB\n",
      "25/04/16 20:51:07 WARN DAGScheduler: Broadcasting large task binary with size 10.6 MiB\n",
      "25/04/16 20:51:08 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:51:09 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:51:10 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:51:10 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/04/16 20:51:10 WARN DAGScheduler: Broadcasting large task binary with size 8.3 MiB\n",
      "25/04/16 20:51:11 WARN DAGScheduler: Broadcasting large task binary with size 10.6 MiB\n",
      "25/04/16 20:51:12 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "[Stage 1596:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "|Customer_ID|cluster|\n",
      "+-----------+-------+\n",
      "|      38664|      0|\n",
      "|      58182|      2|\n",
      "|      22370|      4|\n",
      "|       4925|      1|\n",
      "|       4927|      4|\n",
      "|       1962|      2|\n",
      "|      38686|      2|\n",
      "|      22394|      3|\n",
      "|      58209|      1|\n",
      "|       1964|      1|\n",
      "+-----------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 20:51:12 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans(k=5, seed=42, featuresCol=\"hybrid_vector\", predictionCol=\"cluster\")\n",
    "kmeans_model = kmeans.fit(user_vectors)\n",
    "\n",
    "user_clusters = kmeans_model.transform(user_vectors)\n",
    "user_clusters.select(\"Customer_ID\", \"cluster\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 20:51:13 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/04/16 20:51:13 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/04/16 20:51:14 WARN DAGScheduler: Broadcasting large task binary with size 8.2 MiB\n",
      "25/04/16 20:51:14 WARN DAGScheduler: Broadcasting large task binary with size 8.3 MiB\n",
      "25/04/16 20:51:15 WARN DAGScheduler: Broadcasting large task binary with size 10.5 MiB\n",
      "25/04/16 20:51:15 WARN DAGScheduler: Broadcasting large task binary with size 10.6 MiB\n",
      "25/04/16 20:51:16 WARN DAGScheduler: Broadcasting large task binary with size 12.4 MiB\n",
      "25/04/16 20:51:16 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "25/04/16 20:51:17 WARN DAGScheduler: Broadcasting large task binary with size 12.6 MiB\n",
      "[Stage 1615:>                                                       (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------------------+\n",
      "|cluster|Products|avg_rating        |\n",
      "+-------+--------+------------------+\n",
      "|0      |317     |4.168067226890757 |\n",
      "|0      |121     |3.8320964749536177|\n",
      "|0      |123     |3.813053097345133 |\n",
      "|0      |124     |3.7895277207392195|\n",
      "|0      |126     |3.7858880778588806|\n",
      "|0      |122     |3.772727272727273 |\n",
      "|0      |128     |3.7676470588235293|\n",
      "|0      |125     |3.7367816091954023|\n",
      "|0      |127     |3.7314629258517034|\n",
      "|0      |120     |3.675704989154013 |\n",
      "+-------+--------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 20:51:19 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Join cluster info back to hybrid_df\n",
    "clustered_data = hybrid_df.join(user_clusters.select(\"Customer_ID\", \"cluster\"), on=\"Customer_ID\")\n",
    "\n",
    "# Most popular products in each cluster (based on avg Combined_Rating or frequency)\n",
    "popular_by_cluster = clustered_data.groupBy(\"cluster\", \"Products\") \\\n",
    "    .agg(F.avg(\"Combined_Rating\").alias(\"avg_rating\")) \\\n",
    "    .orderBy(\"cluster\", F.desc(\"avg_rating\"))\n",
    "\n",
    "popular_by_cluster.show(10, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
