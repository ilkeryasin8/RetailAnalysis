{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = (\n",
    "  '--packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.2 '\n",
    "  'pyspark-shell'\n",
    ")\n",
    "\n",
    "# Spark SQL\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, sum, when, udf, row_number\n",
    "\n",
    "# ML pipelines & features\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer,\n",
    "    OneHotEncoder,\n",
    "    VectorAssembler,\n",
    "    MinMaxScaler,\n",
    "    Word2Vec\n",
    ")\n",
    "\n",
    "# ALS recommendation\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Clustering\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Linear algebra types\n",
    "from pyspark.ml.linalg import Vectors, DenseVector, VectorUDT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/17 22:01:57 WARN Utils: Your hostname, Ilker-MacBook-Pro-2.local resolves to a loopback address: 127.0.0.1; using 172.20.10.2 instead (on interface en0)\n",
      "25/04/17 22:01:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/ilkeryasincakir/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/ilkeryasincakir/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-098784b9-0288-48fe-8a12-2bb880026b05;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.2 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/ilkeryasincakir/Developer/Spark/spark-3.5.3-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 99ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-098784b9-0288-48fe-8a12-2bb880026b05\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/3ms)\n",
      "25/04/17 22:01:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "MONGO_URI = \"mongodb+srv://bigdata:JyRgEeuQ3X0Uz29g@retaildb.e9pmb.mongodb.net/retaildb.transactions?retryWrites=true&w=majority\"\n",
    "\n",
    "# Build or retrieve a Spark session named \"Recommendation\" with tuned resources, parallelism, and Kryo serialization\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Recommendation\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.cores\", \"8\") \\\n",
    "    .config(\"spark.default.parallelism\", \"8\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.3\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1024m\") \\\n",
    "    .config(\"spark.mongodb.input.uri\",  MONGO_URI) \\\n",
    "    .config(\"spark.mongodb.output.uri\", MONGO_URI) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Only log error-level messages to reduce console verbosity\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_mongodb(spark):\n",
    "    \"\"\"Read data from MongoDB using Spark MongoDB connector\"\"\"\n",
    "    print(\"Reading data from MongoDB...\")\n",
    "    \n",
    "    # Read data from MongoDB\n",
    "    df = spark.read \\\n",
    "        .format(\"mongo\") \\\n",
    "        .option(\"database\", \"RetailDB\") \\\n",
    "        .option(\"collection\", \"transactions\") \\\n",
    "        .load()\n",
    "    \n",
    "    print(f\"Successfully read {df.count()} records from MongoDB\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from MongoDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read 301171 records from MongoDB\n",
      "root\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Customer_ID: double (nullable = true)\n",
      " |-- Customer_Segment: string (nullable = true)\n",
      " |-- Feedback: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Income: double (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- Payment_Method: string (nullable = true)\n",
      " |-- Product_Brand: string (nullable = true)\n",
      " |-- Product_Category: string (nullable = true)\n",
      " |-- Product_Type: string (nullable = true)\n",
      " |-- Ratings: double (nullable = true)\n",
      " |-- Shipping_Method: string (nullable = true)\n",
      " |-- Total_Purchases: double (nullable = true)\n",
      " |-- Transaction_ID: integer (nullable = true)\n",
      " |-- Year: double (nullable = true)\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- products: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load data from a CSV file into a Spark DataFrame, using the first row as column headers\n",
    "# df = spark.read \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .csv(\"new_cleaned_data.csv\")\n",
    "\n",
    "df = read_from_mongodb(spark)\n",
    "# Show the DataFrame’s schema (column names and types) to confirm successful load and structure\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+-------+---------------+------+--------------------+--------------------+-----------+-------+------+----------------+----------------+-------------+------------+---------------+--------------+----------+--------------+---------------+\n",
      "| Age|Income|  Month|Ratings|Total_Purchases|  Year|                 _id|            products|Customer_ID|Country|Gender|Customer_Segment|Product_Category|Product_Brand|Product_Type|Shipping_Method|Payment_Method|product_id|Transaction_ID|Feedback_Rating|\n",
      "+----+------+-------+-------+---------------+------+--------------------+--------------------+-----------+-------+------+----------------+----------------+-------------+------------+---------------+--------------+----------+--------------+---------------+\n",
      "|42.0|   2.2|January|    5.0|            5.0|2024.0|{680144a6a7ade10f...|             Science|    12573.0|    4.0|   1.0|             2.0|             3.0|          9.0|         2.0|            0.0|           0.0|      34.0|      274095.0|            4.0|\n",
      "|42.0|   2.2|   July|    4.0|            8.0|2023.0|{680144a6a7ade10f...|               Books|    12573.0|    4.0|   1.0|             2.0|             3.0|          9.0|        17.0|            1.0|           0.0|     186.0|      164312.0|            3.0|\n",
      "|42.0|   2.2|October|    2.0|            2.0|2023.0|{680144a6a7ade10f...|               Boots|    12573.0|    4.0|   1.0|             2.0|             2.0|         11.0|         8.0|            1.0|           1.0|      87.0|      106997.0|            2.0|\n",
      "|24.0|   2.0|January|    4.0|            7.0|2023.0|{680144a6a7ade10f...|Stainless steel r...|    60104.0|    0.0|   1.0|             0.0|             0.0|         15.0|        12.0|            0.0|           0.0|     138.0|       40596.0|            3.0|\n",
      "|24.0|   2.0|  March|    5.0|            1.0|2023.0|{680144a6a7ade10f...|      Instant coffee|    60104.0|    0.0|   1.0|             0.0|             1.0|         12.0|        16.0|            1.0|           0.0|     174.0|      232191.0|            4.0|\n",
      "+----+------+-------+-------+---------------+------+--------------------+--------------------+-----------+-------+------+----------------+----------------+-------------+------------+---------------+--------------+----------+--------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the string columns to convert into numeric indices (tweak this list to match your dataset)\n",
    "string_columns = [\n",
    "    \"Customer_ID\", \n",
    "    \"Country\", \n",
    "    \"Gender\", \n",
    "    \"Customer_Segment\",\n",
    "    \"Product_Category\", \n",
    "    \"Product_Brand\", \n",
    "    \"Product_Type\",\n",
    "    \"Shipping_Method\", \n",
    "    \"Payment_Method\",\n",
    "    \"products\", \n",
    "    \"Transaction_ID\"\n",
    "]\n",
    "\n",
    "# Create a StringIndexer stage for each column to map strings → indices, using handleInvalid=\"keep\" \n",
    "# so unseen labels won’t break the pipeline\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=col + \"_index\", handleInvalid=\"keep\")\n",
    "    for col in string_columns\n",
    "]\n",
    "\n",
    "# Chain all indexers into a single Pipeline, fit it on df, and transform to get indexed_df\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "indexed_df = pipeline.fit(df).transform(df)\n",
    "\n",
    "# Build a Python dict mapping each numeric product index back to its original name\n",
    "product_id_to_name = (\n",
    "    indexed_df\n",
    "    .select(\"products_index\", \"products\")\n",
    "    .distinct()\n",
    "    .rdd\n",
    "    .map(lambda r: (int(r[\"products_index\"]), r[\"products\"]))\n",
    "    .collectAsMap()\n",
    ")\n",
    "\n",
    "# Replace each original string column with its indexed version:\n",
    "# – For 'products', keep the original string and rename products_index → product_id\n",
    "# – For all other columns, drop the string col and rename the index col back to the original name\n",
    "for col_name in string_columns:\n",
    "    if col_name == \"products\":\n",
    "        indexed_df = indexed_df.withColumnRenamed(\"products_index\", \"product_id\")\n",
    "    else:\n",
    "        indexed_df = (\n",
    "            indexed_df\n",
    "            .drop(col_name)\n",
    "            .withColumnRenamed(f\"{col_name}_index\", col_name)\n",
    "        )\n",
    "\n",
    "\n",
    "# Convert categorical feedback into a numeric rating scale (Bad→1.0, Average→2.0, Good→3.0, Excellent→4.0)\n",
    "indexed_df = indexed_df.withColumn(\n",
    "    \"Feedback_Rating\",\n",
    "    when(col(\"Feedback\") == \"Bad\", 1.0)\n",
    "    .when(col(\"Feedback\") == \"Average\", 2.0)\n",
    "    .when(col(\"Feedback\") == \"Good\", 3.0)\n",
    "    .when(col(\"Feedback\") == \"Excellent\", 4.0)\n",
    "    .otherwise(None)  # Assign null if feedback is missing or unexpected\n",
    ")\n",
    "\n",
    "# Drop the now-redundant original 'Feedback' column\n",
    "indexed_df = indexed_df.drop(\"Feedback\")\n",
    "\n",
    "# Display a sample of the transformed DataFrame to verify changes\n",
    "indexed_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert month names or numeric strings into a numeric Month_Index (1–12), defaulting to 0.0 for any unexpected values\n",
    "indexed_df = indexed_df.withColumn(\n",
    "    \"Month_Index\",\n",
    "    when((col(\"Month\") == \"January\")   | (col(\"Month\") == \"1.0\"),  1)\n",
    "   .when((col(\"Month\") == \"February\")  | (col(\"Month\") == \"2.0\"),  2)\n",
    "   .when((col(\"Month\") == \"March\")     | (col(\"Month\") == \"3.0\"),  3)\n",
    "   .when((col(\"Month\") == \"April\")     | (col(\"Month\") == \"4.0\"),  4)\n",
    "   .when((col(\"Month\") == \"May\")       | (col(\"Month\") == \"5.0\"),  5)\n",
    "   .when((col(\"Month\") == \"June\")      | (col(\"Month\") == \"6.0\"),  6)\n",
    "   .when((col(\"Month\") == \"July\")      | (col(\"Month\") == \"7.0\"),  7)\n",
    "   .when((col(\"Month\") == \"August\")    | (col(\"Month\") == \"8.0\"),  8)\n",
    "   .when((col(\"Month\") == \"September\") | (col(\"Month\") == \"9.0\"),  9)\n",
    "   .when((col(\"Month\") == \"October\")   | (col(\"Month\") == \"10.0\"), 10)\n",
    "   .when((col(\"Month\") == \"November\")  | (col(\"Month\") == \"11.0\"), 11)\n",
    "   .when((col(\"Month\") == \"December\")  | (col(\"Month\") == \"12.0\"), 12)\n",
    "   .otherwise(0.0)\n",
    ")\n",
    "\n",
    "# Remove the original 'Month' column now that Month_Index is available\n",
    "indexed_df = indexed_df.drop(\"Month\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:>                                                         (0 + 3) / 3]\r"
     ]
    }
   ],
   "source": [
    "# Count and display nulls in each column to assess data completeness\n",
    "null_counts = indexed_df.select([\n",
    "    sum(col(c).isNull().cast(\"int\")).alias(c) \n",
    "    for c in indexed_df.columns\n",
    "])\n",
    "null_counts.show()\n",
    "\n",
    "\n",
    "# Cast key fields to numeric types for downstream processing\n",
    "indexed_df = indexed_df \\\n",
    "    .withColumn(\"Ratings\",      col(\"Ratings\").cast(\"Double\")) \\\n",
    "    .withColumn(\"Age\",          col(\"Age\").cast(\"Int\")) \\\n",
    "    .withColumn(\"Year\",         col(\"Year\").cast(\"Int\")) \\\n",
    "    .withColumn(\"Income\",       col(\"Income\").cast(\"Double\")) \\\n",
    "    .withColumn(\"Customer_ID\",  col(\"Customer_ID\").cast(\"Int\")) \\\n",
    "    .withColumn(\"product_id\",   col(\"product_id\").cast(\"Int\"))\n",
    "\n",
    "# Define mixing weight for explicit ratings vs. feedback-derived ratings\n",
    "alpha = 0.5\n",
    "\n",
    "# Compute a combined rating: blend the original Ratings and the Feedback_Rating\n",
    "indexed_df = indexed_df.withColumn(\n",
    "    \"Combined_Rating\",\n",
    "    alpha * col(\"Ratings\") + (1 - alpha) * col(\"Feedback_Rating\")\n",
    ")\n",
    "\n",
    "# Quick peek at the three rating columns to verify the new calculation\n",
    "indexed_df.select(\"Ratings\", \"Feedback_Rating\", \"Combined_Rating\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a clear, logical ordering of columns to improve readability and downstream processing\n",
    "reordered_cols = [\n",
    "    \"Customer_ID\", \n",
    "    \"Age\", \n",
    "    \"Gender\", \n",
    "    \"Country\", \n",
    "    \"Income\", \n",
    "    \"Customer_Segment\",\n",
    "    \"Transaction_ID\", \n",
    "    \"Year\", \n",
    "    \"Month_Index\", \n",
    "    \"Payment_Method\", \n",
    "    \"Shipping_Method\", \n",
    "    \"Total_Purchases\", \n",
    "    \"Products\",\n",
    "    \"Product_Category\", \n",
    "    \"Product_Brand\", \n",
    "    \"Product_Type\",\n",
    "    \"Ratings\", \n",
    "    \"Feedback_Rating\",\n",
    "    \"Combined_Rating\"\n",
    "]\n",
    "\n",
    "# Reorder the DataFrame columns according to the specified sequence\n",
    "indexed_df = indexed_df.select(reordered_cols)\n",
    "\n",
    "# Show a quick sample to verify that columns have been reordered correctly\n",
    "indexed_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation Phase 1\n",
    "\n",
    "1. Generate Recommendations with Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index the 'Products' column to numeric values, keeping unseen labels\n",
    "productIndexer = StringIndexer(\n",
    "    inputCol=\"Products\", \n",
    "    outputCol=\"Products_Index\", \n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "indexed_df = productIndexer.fit(indexed_df).transform(indexed_df)\n",
    "\n",
    "# Split the data into 80% training and 20% test sets (seed for reproducibility)\n",
    "train, test = indexed_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Configure an ALS model for collaborative filtering on customer–product interactions\n",
    "als = ALS(\n",
    "    userCol=\"Customer_ID\",         # customer identifier\n",
    "    itemCol=\"Products_Index\",      # product identifier (indexed)\n",
    "    ratingCol=\"Ratings\",           # explicit rating column\n",
    "    rank=10,                       # number of latent factors\n",
    "    maxIter=10,                    # number of optimization iterations\n",
    "    regParam=0.1,                  # regularization parameter\n",
    "    coldStartStrategy=\"drop\"       # drop NaN predictions during evaluation\n",
    ")\n",
    "\n",
    "# Train the ALS model on the training set\n",
    "model = als.fit(train)\n",
    "\n",
    "# Generate predicted ratings for the test set\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Produce top-5 recommendations for every user\n",
    "userRecs = model.recommendForAllUsers(5)\n",
    "userRecs.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure an evaluator to measure the model’s prediction accuracy using RMSE\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",            # use root-mean-square error\n",
    "    labelCol=\"Ratings\",           # ground-truth ratings column\n",
    "    predictionCol=\"prediction\"    # model’s predicted ratings\n",
    ")\n",
    "\n",
    "# Compute and display the RMSE on the test dataset\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error (RMSE):\", rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Generate Recommendations with Feedbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a separate ALS model to learn from implicit feedback (Feedback_Rating) instead of explicit Ratings\n",
    "als_fb = ALS(\n",
    "    userCol=\"Customer_ID\",          # customer identifier\n",
    "    itemCol=\"Products_Index\",       # product identifier (indexed)\n",
    "    ratingCol=\"Feedback_Rating\",    # use the feedback-derived rating as the target\n",
    "    rank=10,                        # number of latent factors\n",
    "    maxIter=10,                     # iterations for matrix factorization\n",
    "    regParam=0.1,                   # regularization strength\n",
    "    coldStartStrategy=\"drop\"        # drop NaN predictions during recommendation\n",
    ")\n",
    "\n",
    "# Train the feedback-based ALS model on the training set\n",
    "als_model_fb = als_fb.fit(train)\n",
    "\n",
    "# Generate top-5 recommendations per user based on feedback patterns\n",
    "userRecs = als_model_fb.recommendForAllUsers(5)\n",
    "userRecs.show(5, truncate=False)\n",
    "\n",
    "# Create predictions on the test set to evaluate feedback-based model quality\n",
    "predictions = als_model_fb.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RMSE evaluator comparing predicted vs. actual feedback ratings\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",             # measure root-mean-square error\n",
    "    labelCol=\"Feedback_Rating\",    # true feedback values\n",
    "    predictionCol=\"prediction\"     # model’s feedback predictions\n",
    ")\n",
    "\n",
    "# Compute and print RMSE for the feedback-based ALS model\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error (RMSE) for feedback-based model:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Generate Recommendations with Combined Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure an ALS model using the blended Combined_Rating (explicit + feedback)\n",
    "als_combined = ALS(\n",
    "    userCol=\"Customer_ID\",           # customer identifier\n",
    "    itemCol=\"Products_Index\",        # product identifier (indexed)\n",
    "    ratingCol=\"Combined_Rating\",     # use the precomputed weighted rating\n",
    "    rank=10,                         # number of latent factors\n",
    "    maxIter=10,                      # optimization iterations\n",
    "    regParam=0.1,                    # regularization strength to prevent overfitting\n",
    "    coldStartStrategy=\"drop\"         # drop NaN predictions during evaluation\n",
    ")\n",
    "\n",
    "# Train the combined-rating ALS model on the training dataset\n",
    "als_model_combined = als_combined.fit(train)\n",
    "\n",
    "# Generate top-5 product recommendations per user based on the combined model\n",
    "userRecs = als_model_combined.recommendForAllUsers(5)\n",
    "userRecs.show(5, truncate=False)\n",
    "\n",
    "# Produce predictions on the test set to assess model performance\n",
    "predictions = als_model_combined.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up an evaluator to compute RMSE between predicted and actual Combined_Rating\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",             # root-mean-square error metric\n",
    "    labelCol=\"Combined_Rating\",    # ground-truth combined rating\n",
    "    predictionCol=\"prediction\"     # model’s predicted rating\n",
    ")\n",
    "\n",
    "# Calculate and print the RMSE for the combined-rating model\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error (RMSE) for combined model:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation Phase 2\n",
    "\n",
    "1. User Profile Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather each user’s core attributes (ID, demographics, segment) exactly once\n",
    "distinct_users = indexed_df.select(\n",
    "    \"Customer_ID\", \n",
    "    \"Age\", \n",
    "    \"Gender\", \n",
    "    \"Country\", \n",
    "    \"Income\", \n",
    "    \"Customer_Segment\"\n",
    ").distinct()\n",
    "\n",
    "# Combine the selected fields into a single feature vector for downstream modeling\n",
    "user_profile_vector = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"Customer_ID\",\n",
    "        \"Age\", \n",
    "        \"Gender\", \n",
    "        \"Country\", \n",
    "        \"Income\", \n",
    "        \"Customer_Segment\"\n",
    "    ],\n",
    "    outputCol=\"user_profile_vector\"\n",
    ").transform(distinct_users)\n",
    "\n",
    "# Preview the user IDs alongside their assembled feature vectors\n",
    "user_profile_vector.select(\n",
    "    \"Customer_ID\", \n",
    "    \"user_profile_vector\"\n",
    ").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Product Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble product-related fields into a single feature vector for downstream modeling\n",
    "product_vector = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"Transaction_ID\",    # unique transaction identifier\n",
    "        \"Product_Category\",  # categorical index for product category\n",
    "        \"Product_Brand\",     # categorical index for product brand\n",
    "        \"Product_Type\",      # categorical index for product type\n",
    "        \"Products_Index\"     # numerical index for the actual product\n",
    "    ],\n",
    "    outputCol=\"product_vector\"\n",
    ").transform(indexed_df)\n",
    "\n",
    "# Show a few products alongside their assembled feature vectors\n",
    "product_vector.select(\"Products\", \"product_vector\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Transaction Vector  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble transaction-level features—including time and combined rating—into one vector\n",
    "transaction_vector = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"Transaction_ID\",    # unique transaction identifier\n",
    "        \"Customer_ID\",       # customer identifier\n",
    "        \"Month_Index\",       # numerical month value\n",
    "        \"Year\",              # transaction year\n",
    "        \"Combined_Rating\"    # blended rating metric\n",
    "    ],\n",
    "    outputCol=\"transaction_vector\"\n",
    ").transform(indexed_df)\n",
    "\n",
    "# Preview a sample of transaction vectors for validation\n",
    "transaction_vector.select(\"Transaction_ID\", \"transaction_vector\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale user profile vectors to [0,1] range for uniform weighting\n",
    "scaler_user = MinMaxScaler(\n",
    "    inputCol=\"user_profile_vector\", \n",
    "    outputCol=\"user_profile_scaled\"\n",
    ")\n",
    "scaler_model_user = scaler_user.fit(user_profile_vector)\n",
    "user_profile_scaled = scaler_model_user.transform(user_profile_vector)\n",
    "user_profile_scaled.select(\"Customer_ID\", \"user_profile_scaled\").show(5, truncate=False)\n",
    "\n",
    "# Scale product feature vectors to [0,1] to align with other scaled inputs\n",
    "scaler_product = MinMaxScaler(\n",
    "    inputCol=\"product_vector\", \n",
    "    outputCol=\"product_vector_scaled\"\n",
    ")\n",
    "scaler_model_product = scaler_product.fit(product_vector)\n",
    "product_scaled = scaler_model_product.transform(product_vector)\n",
    "product_scaled.select(\"Products\", \"product_vector_scaled\").show(5, truncate=False)\n",
    "\n",
    "# Scale transaction vectors (including combined rating and time features) to [0,1]\n",
    "scaler_transaction = MinMaxScaler(\n",
    "    inputCol=\"transaction_vector\", \n",
    "    outputCol=\"transaction_vector_scaled\"\n",
    ")\n",
    "scaler_model_transaction = scaler_transaction.fit(transaction_vector)\n",
    "transaction_vector_scaled = scaler_model_transaction.transform(transaction_vector)\n",
    "transaction_vector_scaled.select(\"Transaction_ID\", \"transaction_vector_scaled\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the scaled user profiles\n",
    "user_profile_scaled = user_profile_scaled.select(\n",
    "    \"Customer_ID\", \n",
    "    \"user_profile_scaled\"\n",
    ")\n",
    "\n",
    "# Keep relevant product features including its scaled vector\n",
    "product_scaled = product_scaled.select(\n",
    "    \"Transaction_ID\", \n",
    "    \"Products\", \n",
    "    \"product_vector_scaled\"\n",
    ")\n",
    "\n",
    "# Select transaction features plus combined rating for later filtering/evaluation\n",
    "transaction_scaled = transaction_vector_scaled.select(\n",
    "    \"Transaction_ID\", \n",
    "    \"Customer_ID\", \n",
    "    \"transaction_vector_scaled\", \n",
    "    \"Combined_Rating\"\n",
    ")\n",
    "\n",
    "# Join transaction records with their corresponding user profiles on Customer_ID\n",
    "user_transaction_joined = transaction_scaled.join(\n",
    "    user_profile_scaled, \n",
    "    on=\"Customer_ID\", \n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Next, bring in product info by joining on Transaction_ID\n",
    "hybrid_joined = user_transaction_joined.join(\n",
    "    product_scaled, \n",
    "    on=\"Transaction_ID\", \n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Assemble a single \"hybrid_vector\" from user profile, transaction, and product vectors\n",
    "hybrid_assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"user_profile_scaled\", \n",
    "        \"transaction_vector_scaled\", \n",
    "        \"product_vector_scaled\"\n",
    "    ],\n",
    "    outputCol=\"hybrid_vector\"\n",
    ")\n",
    "hybrid_df = hybrid_assembler.transform(hybrid_joined)\n",
    "\n",
    "# Preview hybrid feature vectors alongside customer, product, and rating\n",
    "hybrid_df.select(\n",
    "    \"Customer_ID\", \n",
    "    \"Products\", \n",
    "    \"hybrid_vector\", \n",
    "    \"Combined_Rating\"\n",
    ").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique customer hybrid vectors to avoid duplicates\n",
    "user_vectors = hybrid_df.select(\"Customer_ID\", \"hybrid_vector\").distinct()\n",
    "\n",
    "# Group all hybrid vectors by product to aggregate user interactions per product\n",
    "vector_list_df = hybrid_df.groupBy(\"Products\").agg(\n",
    "    F.first(\"Transaction_ID\").alias(\"Transaction_ID\"),\n",
    "    F.first(\"Customer_ID\").alias(\"Customer_ID\"),\n",
    "    F.collect_list(\"hybrid_vector\").alias(\"hybrid_vector_list\")\n",
    ")\n",
    "\n",
    "# UDF to compute the element-wise average of a list of Spark vectors\n",
    "def average_vectors(vectors):\n",
    "    if not vectors:\n",
    "        return None\n",
    "    arrays = [np.array(v.toArray()) for v in vectors]\n",
    "    return np.mean(arrays, axis=0).tolist()\n",
    "\n",
    "average_vectors_udf = F.udf(average_vectors, ArrayType(DoubleType()))\n",
    "\n",
    "# Apply the averaging UDF to get a raw array representation of each product’s vector\n",
    "vector_list_df = vector_list_df.withColumn(\n",
    "    \"avg_vector_array\", \n",
    "    average_vectors_udf(\"hybrid_vector_list\")\n",
    ")\n",
    "\n",
    "# UDF to convert a Python list back into a Spark DenseVector\n",
    "def array_to_vector(arr):\n",
    "    if arr is None:\n",
    "        return None\n",
    "    return DenseVector(arr)\n",
    "\n",
    "array_to_vector_udf = F.udf(array_to_vector, VectorUDT())\n",
    "\n",
    "# Turn the averaged array into a DenseVector and select final product embeddings\n",
    "product_vectors = vector_list_df.withColumn(\n",
    "    \"product_vector\", \n",
    "    array_to_vector_udf(\"avg_vector_array\")\n",
    ").select(\"Products\", \"product_vector\")\n",
    "\n",
    "# Show the resulting product vectors\n",
    "product_vectors.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper to compute cosine similarity between two numeric vectors:\n",
    "# returns None if either vector is missing, 0.0 if a vector has zero magnitude, \n",
    "# otherwise the normalized dot product.\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    if vec1 is None or vec2 is None:\n",
    "        return None\n",
    "    # Convert inputs (e.g., Spark vectors) to NumPy arrays for linear algebra\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    # Compute L2 norms; guard against division by zero\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "    # Return the cosine similarity: dot(vec1, vec2) / (||vec1|| * ||vec2||)\n",
    "    return float(np.dot(vec1, vec2) / (norm1 * norm2))\n",
    "\n",
    "# Register the Python function as a Spark UDF that returns a DoubleType\n",
    "cosine_sim_udf = udf(cosine_similarity, DoubleType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all possible user–product pairs for scoring\n",
    "user_product_scores = user_vectors.crossJoin(product_vectors)\n",
    "\n",
    "# Compute cosine similarity between each user’s hybrid vector and each product vector\n",
    "user_product_scores = user_product_scores.withColumn(\n",
    "    \"similarity\",\n",
    "    cosine_sim_udf(\"hybrid_vector\", \"product_vector\")\n",
    ")\n",
    "\n",
    "# Define a window to rank products per user by descending similarity\n",
    "windowSpec = Window.partitionBy(\"Customer_ID\").orderBy(F.desc(\"similarity\"))\n",
    "\n",
    "# Assign ranks and keep only the top 5 recommendations per user\n",
    "top_n_recommendations = (\n",
    "    user_product_scores\n",
    "    .withColumn(\"rank\", row_number().over(windowSpec))\n",
    "    .filter(\"rank <= 5\")\n",
    ")\n",
    "\n",
    "# Show user-wise top-5 products with their similarity scores\n",
    "top_n_recommendations.select(\"Customer_ID\", \"Products\", \"similarity\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster users into 5 segments based on their hybrid behavior vectors\n",
    "kmeans = KMeans(\n",
    "    k=5, \n",
    "    seed=42, \n",
    "    featuresCol=\"hybrid_vector\", \n",
    "    predictionCol=\"cluster\"\n",
    ")\n",
    "kmeans_model = kmeans.fit(user_vectors)\n",
    "\n",
    "# Attach cluster labels to each user\n",
    "user_clusters = kmeans_model.transform(user_vectors)\n",
    "user_clusters.select(\"Customer_ID\", \"cluster\").show(10)\n",
    "\n",
    "# Merge cluster assignments back into the full hybrid DataFrame\n",
    "clustered_data = hybrid_df.join(\n",
    "    user_clusters.select(\"Customer_ID\", \"cluster\"),\n",
    "    on=\"Customer_ID\"\n",
    ")\n",
    "\n",
    "# Within each cluster, find the products with highest average Combined_Rating\n",
    "popular_by_cluster = (\n",
    "    clustered_data\n",
    "    .groupBy(\"cluster\", \"Products\")\n",
    "    .agg(F.avg(\"Combined_Rating\").alias(\"avg_rating\"))\n",
    "    .orderBy(\"cluster\", F.desc(\"avg_rating\"))\n",
    ")\n",
    "\n",
    "# Display the top products per user cluster\n",
    "popular_by_cluster.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Based Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Build user purchase sequences\n",
    "window_spec = Window.partitionBy(\"Customer_ID\") \\\n",
    "                    .orderBy(\"Year\", \"Month_Index\", \"Transaction_ID\")\n",
    "\n",
    "user_sequences = (\n",
    "    indexed_df\n",
    "      .withColumn(\"ordered_products\", F.collect_list(\"Products\").over(window_spec))\n",
    "      .groupBy(\"Customer_ID\")\n",
    "      .agg(F.max(\"ordered_products\").alias(\"product_sequence\"))\n",
    "      .withColumn(\"seq_length\", F.size(\"product_sequence\"))\n",
    "      .filter(F.col(\"seq_length\") >= 2)\n",
    ")\n",
    "\n",
    "# 2) Prepare for Word2Vec\n",
    "user_sequences = user_sequences.withColumn(\n",
    "    \"product_sequence_str\",\n",
    "    F.expr(\"transform(product_sequence, x -> cast(x as string))\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Train Word2Vec\n",
    "w2v_model = Word2Vec(\n",
    "    vectorSize=50, minCount=1,\n",
    "    inputCol=\"product_sequence_str\",\n",
    "    outputCol=\"product_embedding\"\n",
    ").fit(user_sequences)\n",
    "\n",
    "product_embeddings = w2v_model.getVectors()  # [word, vector]\n",
    "\n",
    "# 4) Build product→category map\n",
    "prod_cat_map = (\n",
    "    indexed_df\n",
    "      .select(\"Products\", \"Product_Category\")\n",
    "      .distinct()\n",
    "      .rdd\n",
    "      .map(lambda r: (r[\"Products\"], r[\"Product_Category\"]))\n",
    "      .collectAsMap()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Leave‑one‑out evaluation counting same‑category hits\n",
    "top_k = 5\n",
    "eval_results = []\n",
    "\n",
    "for row in user_sequences.select(\"Customer_ID\",\"product_sequence\").collect():\n",
    "    user_id     = row[\"Customer_ID\"]\n",
    "    seq         = row[\"product_sequence\"]\n",
    "    train_seq   = seq[:-1]\n",
    "    actual_item = seq[-1]\n",
    "    actual_cat  = prod_cat_map[actual_item]\n",
    "\n",
    "    # 5.1) Avg embedding\n",
    "    vecs = (\n",
    "        product_embeddings\n",
    "          .filter(F.col(\"word\").isin([str(p) for p in train_seq]))\n",
    "          .select(\"vector\")\n",
    "          .rdd.map(lambda r: np.array(r[0].toArray()))\n",
    "          .collect()\n",
    "    )\n",
    "    avg_vec = Vectors.dense(np.mean(vecs, axis=0).tolist())\n",
    "\n",
    "    # 5.2) Top‑K recommendations\n",
    "    sims = (\n",
    "        w2v_model\n",
    "          .findSynonyms(avg_vec, top_k)\n",
    "          .withColumnRenamed(\"word\", \"predicted\")\n",
    "          .select(\"predicted\",\"similarity\")\n",
    "          .toPandas()\n",
    "    )\n",
    "    preds = sims[\"predicted\"].tolist()   # still strings\n",
    "\n",
    "    # 5.3) Count how many share the actual category\n",
    "    pred_cats = [prod_cat_map[p] for p in preds]\n",
    "    cat_hits  = len([c for c in pred_cats if c == actual_cat])\n",
    "\n",
    "    eval_results.append((user_id, actual_item, actual_cat, preds, cat_hits))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Convert to DataFrame & compute metrics\n",
    "eval_df = spark.createDataFrame(\n",
    "    eval_results,\n",
    "    schema=[\"Customer_ID\",\"actual_item\",\"actual_cat\",\"preds\",\"cat_hits\"]\n",
    ")\n",
    "\n",
    "avg_cat_hits = eval_df.agg(F.avg(\"cat_hits\")).first()[0]\n",
    "cat_hit_rate = avg_cat_hits / top_k\n",
    "\n",
    "print(f\"Avg. category hits in top-{top_k}: {avg_cat_hits:.2f}\")\n",
    "print(f\"Category hit rate@{top_k}: {cat_hit_rate:.2%}\")\n",
    "\n",
    "# 7) Inspect examples\n",
    "eval_df.show(10, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
